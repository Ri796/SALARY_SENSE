{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5twWLfCXYm-"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install Libraries\n",
        "!pip install streamlit pandas scikit-learn xgboost joblib shap pyngrok -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Train and Save the Model\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "KAgDxCQNdGz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "try:\n",
        "    data = pd.read_csv('adult 3.csv')\n",
        "    print(\"Successfully loaded 'adult 3.csv'.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: 'adult 3.csv' not found. Please upload it.\")\n",
        "    raise\n",
        "\n"
      ],
      "metadata": {
        "id": "imGA2HJ1mR8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Data Cleaning\n",
        "data.replace({' ?': 'Others', '?': 'Others'}, inplace=True)\n",
        "data = data[data['workclass'] != 'Without-pay']\n",
        "data = data[data['workclass'] != 'Never-worked']\n",
        "if 'education' in data.columns:\n",
        "    data = data.drop(columns=['education'])\n",
        "\n"
      ],
      "metadata": {
        "id": "tHaJmND0mXnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standalone Cell for Outlier Visualization with Box Plots\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#  Define the columns to check for outliers\n",
        "numerical_cols_to_check = ['age', 'hours-per-week', 'capital-gain']\n",
        "\n",
        "#  Creating \"Before\" plots\n",
        "print(\"Visualizing Outliers: BEFORE Cleaning \")\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, axes = plt.subplots(1, len(numerical_cols_to_check), figsize=(18, 6))\n",
        "fig.suptitle('Box Plots of Key Features - BEFORE Outlier Removal', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, col in enumerate(numerical_cols_to_check):\n",
        "    sns.boxplot(y=data_original[col], ax=axes[i], color='skyblue')\n",
        "    axes[i].set_title(f'Distribution of {col}', fontsize=12)\n",
        "    axes[i].set_ylabel('') # Clean up y-axis label\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Apply the same cleaning rules from your notebook\n",
        "data_cleaned = data_original.copy()\n",
        "data_cleaned = data_cleaned[(data_cleaned['age'] <= 75) & (data_cleaned['age'] >= 17)]\n",
        "# You might have other filtering rules for 'hours-per-week' etc. Add them here if needed.\n",
        "# Example: data_cleaned = data_cleaned[data_cleaned['hours-per-week'] <= 60]\n",
        "\n",
        "\n",
        "#  Create \"After\" plots\n",
        "print(\"\\n Visualizing Outliers: AFTER Cleaning \")\n",
        "fig, axes = plt.subplots(1, len(numerical_cols_to_check), figsize=(18, 6))\n",
        "fig.suptitle('Box Plots of Key Features - AFTER Outlier Removal', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, col in enumerate(numerical_cols_to_check):\n",
        "    sns.boxplot(y=data_cleaned[col], ax=axes[i], color='lightgreen')\n",
        "    axes[i].set_title(f'Distribution of {col}', fontsize=12)\n",
        "    axes[i].set_ylabel('')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uw1-sMbBmbwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Target Transformation\n",
        "data['income'] = data['income'].apply(lambda x: 1 if str(x).strip() == '>50K' else 0)\n",
        "\n",
        "# 4. Define Features (X) and Target (y)\n",
        "X = data.drop(columns=['income'])\n",
        "y = data['income']\n",
        "\n",
        "# 5. Identify feature types\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# 6. Create the Preprocessing Pipeline using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# 7. Create the Full Final Pipeline\n",
        "final_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# 8. Train the Full Pipeline\n",
        "print(\"Training the full pipeline...\")\n",
        "final_pipeline.fit(X, y)\n",
        "print(\"--- Model Training Complete ---\")\n",
        "\n",
        "# 9. Save the final pipeline object\n",
        "joblib.dump(final_pipeline, '/content/income_prediction_pipeline.joblib')\n",
        "print(\"Final verified pipeline saved successfully.\")"
      ],
      "metadata": {
        "id": "cL0LV5FUmaGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# 1. LOAD THE PIPELINE\n",
        "st.set_page_config(page_title=\"Salary Predictor\", page_icon=\"ðŸ’¼\", layout=\"centered\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_pipeline():\n",
        "    \"\"\"Loads the pre-trained pipeline.\"\"\"\n",
        "    return joblib.load('/content/income_prediction_pipeline.joblib')\n",
        "\n",
        "pipeline = load_pipeline()\n",
        "\n",
        "# 2. USER INTERFACE\n",
        "st.title('ðŸ’¼ SALARY_SENSE: Employee Income Predictor')\n",
        "st.write(\"Predict if an employee earns >50K or â‰¤50K using a Gradient Boosting model.\")\n",
        "\n",
        "# We create the input fields based on the columns of a sample row.\n",
        "# This ensures we have all the necessary inputs.\n",
        "sample_columns = ['age', 'workclass', 'fnlwgt', 'educational-num', 'marital-status',\n",
        "                  'occupation', 'relationship', 'race', 'gender', 'capital-gain',\n",
        "                  'capital-loss', 'hours-per-week', 'native-country']\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"ðŸ‘¤ Applicant Details\")\n",
        "    input_dict = {}\n",
        "\n",
        "    # Create inputs for all features the model was trained on\n",
        "    input_dict['age'] = st.slider(\"Age\", 17, 90, 30)\n",
        "    input_dict['workclass'] = st.selectbox(\"Work Class\", ['Private', 'Self-emp-not-inc', 'Local-gov', 'Federal-gov', 'State-gov', 'Self-emp-inc', 'Others'])\n",
        "    input_dict['fnlwgt'] = st.number_input(\"fnlwgt (Final Weight)\", value=180000)\n",
        "    input_dict['educational-num'] = st.slider(\"Education Years\", 1, 16, 10)\n",
        "    input_dict['marital-status'] = st.selectbox('Marital Status', ['Married-civ-spouse', 'Never-married', 'Divorced', 'Separated', 'Widowed', 'Married-spouse-absent', 'Married-AF-spouse'])\n",
        "    input_dict['occupation'] = st.selectbox(\"Occupation\", ['Prof-specialty', 'Craft-repair', 'Exec-managerial', 'Adm-clerical', 'Sales', 'Other-service', 'Machine-op-inspct', 'Others', 'Transport-moving', 'Handlers-cleaners', 'Farming-fishing', 'Tech-support', 'Protective-serv', 'Priv-house-serv', 'Armed-Forces'])\n",
        "    input_dict['relationship'] = st.selectbox('Relationship', ['Husband', 'Not-in-family', 'Own-child', 'Unmarried', 'Wife', 'Other-relative'])\n",
        "    input_dict['race'] = st.selectbox('Race', ['White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'])\n",
        "    input_dict['gender'] = st.selectbox('Gender', ['Male', 'Female'])\n",
        "    input_dict['capital-gain'] = st.number_input(\"Capital Gain\", value=0)\n",
        "    input_dict['capital-loss'] = st.number_input(\"Capital Loss\", value=0)\n",
        "    input_dict['hours-per-week'] = st.slider(\"Hours per Week\", 1, 99, 40)\n",
        "    input_dict['native-country'] = st.selectbox(\"Native Country\", ['United-States', 'Mexico', 'Philippines', 'Germany', 'Canada', 'Puerto-Rico', 'Others'])\n",
        "\n",
        "\n",
        "# 3. PREDICTION LOGIC\n",
        "if st.button('**Predict Income Level**', type='primary'):\n",
        "    input_df = pd.DataFrame([input_dict])\n",
        "\n",
        "    st.write(\"### ðŸ”Ž Input Data\")\n",
        "    st.write(input_df)\n",
        "\n",
        "    try:\n",
        "        prediction = pipeline.predict(input_df)[0]\n",
        "        prediction_proba = pipeline.predict_proba(input_df)[0]\n",
        "\n",
        "        st.header(\"Prediction Result\")\n",
        "        if prediction == 1:\n",
        "            st.success(f'**Income is likely >$50K** (Confidence: {prediction_proba[1]:.2%})')\n",
        "        else:\n",
        "            st.warning(f'**Income is likely â‰¤50K** (Confidence: {prediction_proba[0]:.2%})')\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during prediction: {e}\")"
      ],
      "metadata": {
        "id": "5_MLMplpdJjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standalone Cell for SHAP Analysis (Corrected for Summary Plot)\n",
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"--- Starting Standalone SHAP Analysis ---\")\n",
        "\n",
        "# 1. Load your saved pipeline\n",
        "try:\n",
        "    pipeline = joblib.load('/content/income_prediction_pipeline.joblib')\n",
        "    print(\"* Pipeline loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Pipeline file not found. Please run the training script first.\")\n",
        "    raise\n",
        "\n",
        "# 2. Load a sample of your data\n",
        "try:\n",
        "    data = pd.read_csv('adult 3.csv')\n",
        "    data.replace({' ?': 'Others', '?': 'Others'}, inplace=True)\n",
        "    if 'income' in data.columns:\n",
        "        X_explain = data.drop(columns=['income'])\n",
        "    else:\n",
        "        X_explain = data\n",
        "\n",
        "    X_sample = X_explain.sample(100, random_state=42)\n",
        "    print(f\"* Loaded and prepared a sample of {len(X_sample)} rows to explain.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: 'adult 3.csv' not found. Please upload it for SHAP analysis.\")\n",
        "    raise\n",
        "\n",
        "# 3. Create the SHAP Explainer\n",
        "print(\"Creating SHAP explainer...\")\n",
        "preprocessor = pipeline.named_steps['preprocessor']\n",
        "classifier = pipeline.named_steps['classifier']\n",
        "transformed_background = preprocessor.transform(X_sample).toarray()\n",
        "explainer = shap.TreeExplainer(classifier, transformed_background)\n",
        "print(\"* SHAP explainer created.\")\n",
        "\n",
        "# 4. Calculate SHAP values\n",
        "transformed_sample = preprocessor.transform(X_sample).toarray()\n",
        "shap_values = explainer.shap_values(transformed_sample)\n",
        "print(\"* SHAP values calculated.\")\n",
        "\n",
        "# 5. Generate and Display the Plots\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# PLOT 1: Force Plot for a single prediction\n",
        "print(\"\\n--- SHAP Force Plot (Explaining the first person in the sample) ---\")\n",
        "shap.initjs()\n",
        "display(shap.force_plot(explainer.expected_value, shap_values[0,:], feature_names=feature_names))\n",
        "print(\"Force Plot displayed successfully.\")\n",
        "\n",
        "\n",
        "# PLOT 2: Summary Plot (Beeswarm) for overall feature importance\n",
        "print(\"\\n--- SHAP Summary Plot (Overall Feature Impact) ---\")\n",
        "\n",
        "# THIS IS THE FIX\n",
        "# We pass the TRANSFORMED data to the 'features' argument.\n",
        "# This ensures the dimensions of 'shap_values' and 'features' match perfectly.\n",
        "shap.summary_plot(shap_values, features=transformed_sample, feature_names=feature_names)\n",
        "# END OF THE FIX"
      ],
      "metadata": {
        "id": "eBxe-wHijRQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Launch the Streamlit App\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "NGROK_AUTH_TOKEN = \"30EwBDZQCJvTSQ22N25ei1c7PzH_675q8kJRsw4F23DmYfhm\" # <--- PASTE YOUR TOKEN\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"âœ… Your app is live at: {public_url}\")\n",
        "\n",
        "!streamlit run app.py"
      ],
      "metadata": {
        "id": "qr0zqF83ZDMD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}